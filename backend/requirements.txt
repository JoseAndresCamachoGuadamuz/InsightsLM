# ============================================================================
# InsightsLM Backend Requirements
# Python 3.12.3
# Date: November 7, 2025
# Platform: WSL2 Ubuntu (Production-tested with NVIDIA GTX 1660 SUPER)
# ============================================================================

# ----------------------------------------------------------------------------
# Core Web Framework
# ----------------------------------------------------------------------------
fastapi==0.121.0
uvicorn[standard]==0.38.0
python-dotenv==1.2.1
fastapi-cors==0.0.6
python-multipart==0.0.20
starlette==0.49.3

# ----------------------------------------------------------------------------
# Database & Storage
# ----------------------------------------------------------------------------
sqlalchemy==2.0.44
chromadb==1.3.4
sentence-transformers==5.1.2

# ----------------------------------------------------------------------------
# Audio Processing & Transcription
# CRITICAL: Using faster-whisper (GPU-accelerated) NOT openai-whisper
# ----------------------------------------------------------------------------
faster-whisper==1.2.1
ctranslate2==4.6.0
ffmpeg-python==0.2.0
yt-dlp==2025.10.22

# ----------------------------------------------------------------------------
# PyTorch & GPU Acceleration (CUDA 12.8)
# CRITICAL: These versions are tested and working with GTX 1660 SUPER
# DO NOT upgrade PyTorch without testing - cuDNN compatibility required
# ----------------------------------------------------------------------------
torch==2.9.0
nvidia-cudnn-cu12==9.10.2.21
nvidia-cublas-cu12==12.8.4.1
nvidia-cuda-cupti-cu12==12.8.90
nvidia-cuda-nvrtc-cu12==12.8.93
nvidia-cuda-runtime-cu12==12.8.90
nvidia-cufft-cu12==11.3.3.83
nvidia-cufile-cu12==1.13.1.3
nvidia-curand-cu12==10.3.9.90
nvidia-cusolver-cu12==11.7.3.90
nvidia-cusparse-cu12==12.5.8.93
nvidia-cusparselt-cu12==0.7.1
nvidia-nccl-cu12==2.27.5
nvidia-nvjitlink-cu12==12.8.93
nvidia-nvshmem-cu12==3.3.20
nvidia-nvtx-cu12==12.8.90
triton==3.5.0

# ----------------------------------------------------------------------------
# AI/LLM Providers
# ----------------------------------------------------------------------------
openai==2.7.1
anthropic==0.72.0
google-generativeai==0.8.5
ollama==0.6.0

# ----------------------------------------------------------------------------
# Text-to-Speech
# ----------------------------------------------------------------------------
gTTS==2.5.4

# ----------------------------------------------------------------------------
# Security & Encryption
# ----------------------------------------------------------------------------
pycryptodome==3.23.0

# ----------------------------------------------------------------------------
# Utilities & Platform Detection
# ----------------------------------------------------------------------------
appdirs==1.4.4
py-cpuinfo==9.0.0

# ----------------------------------------------------------------------------
# ML/AI Dependencies (installed by sentence-transformers)
# ----------------------------------------------------------------------------
transformers==4.57.1
tokenizers==0.22.1
safetensors==0.6.2
huggingface-hub==0.36.0
scikit-learn==1.7.2
scipy==1.16.3
numpy==2.3.4

# ----------------------------------------------------------------------------
# HTTP & Networking
# ----------------------------------------------------------------------------
requests==2.32.5
httpx==0.28.1
httpcore==1.0.9

# ----------------------------------------------------------------------------
# Data Processing
# ----------------------------------------------------------------------------
pillow==12.0.0
pydantic==2.12.4

# ============================================================================
# Installation Instructions
# ============================================================================
# 
# 1. Create virtual environment:
#    python3 -m venv venv
#    source venv/bin/activate
#
# 2. Upgrade pip:
#    pip install --upgrade pip
#
# 3. Install requirements:
#    pip install -r requirements.txt
#
# 4. Verify GPU support:
#    python -c "import torch; print(f'CUDA: {torch.cuda.is_available()}')"
#
# 5. Set LD_LIBRARY_PATH (add to ~/.bashrc):
#    export LD_LIBRARY_PATH="$PWD/venv/lib/python3.12/site-packages/nvidia/cudnn/lib:$LD_LIBRARY_PATH"
#
# ============================================================================
# Performance Benchmarks (GTX 1660 SUPER 6GB)
# ============================================================================
#
# Transcription Speed: ~19x real-time
# Example: 16-minute audio â†’ 52 seconds processing time
# Model: small (244M params)
# Compute: int8_float32 (cuDNN disabled for stability)
#
# ============================================================================
