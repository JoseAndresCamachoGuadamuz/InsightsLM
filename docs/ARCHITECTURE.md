# System Architecture

**InsightsLM - AI-Powered Audio Transcription and Analysis Platform**

**Document Version:** 1.1 (Updated with actual system information)  
**Last Updated:** November 4, 2025  
**Status:** Production-Ready

---

## ğŸ“‹ Table of Contents

1. [System Overview](#system-overview)
2. [Architecture Principles](#architecture-principles)
3. [High-Level Architecture](#high-level-architecture)
4. [Technology Stack](#technology-stack)
5. [Component Architecture](#component-architecture)
6. [Data Flow](#data-flow)
7. [Database Design](#database-design)
8. [Security Architecture](#security-architecture)
9. [Deployment Architecture](#deployment-architecture)
10. [Design Decisions](#design-decisions)

---

## ğŸ¯ System Overview

InsightsLM is a hybrid desktop application that combines:
- **Electron-based frontend** (Windows) for cross-platform UI
- **FastAPI backend** (WSL2/Linux) for AI processing
- **Local-first architecture** with encrypted data storage
- **Multi-provider AI** support (Ollama, OpenAI, Anthropic, Google Gemini)

### Key Capabilities
- Audio transcription using OpenAI Whisper
- AI-powered content analysis and summarization
- Custom report generation with templates
- Semantic search with vector embeddings
- Multi-provider AI model selection
- Secure API key management

---

## ğŸ—ï¸ Architecture Principles

### 1. **Separation of Concerns**
- Frontend: UI/UX and user interactions
- Backend: AI processing, data management, API integration

### 2. **Security First**
- AES-256 encryption for API keys
- Isolated backend process
- No sensitive data in frontend code

### 3. **Local-First**
- All data stored locally
- No cloud dependency (except AI providers)
- User owns their data

### 4. **Modularity**
- Service-oriented backend architecture
- Reusable React components
- Clear API boundaries

### 5. **Extensibility**
- Easy to add new AI providers
- Template system for custom reports
- Plugin-ready architecture

---

## ğŸ”„ High-Level Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        WINDOWS ENVIRONMENT                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚                     ELECTRON FRONTEND                       â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚ â”‚
â”‚  â”‚  â”‚  Main Processâ”‚  â”‚   Renderer   â”‚  â”‚   Preload    â”‚    â”‚ â”‚
â”‚  â”‚  â”‚   (main.ts)  â”‚  â”‚   (React)    â”‚  â”‚  (preload.ts)â”‚    â”‚ â”‚
â”‚  â”‚  â”‚   824 lines  â”‚  â”‚  App.tsx     â”‚  â”‚   30 lines   â”‚    â”‚ â”‚
â”‚  â”‚  â”‚              â”‚  â”‚  1,848 lines â”‚  â”‚              â”‚    â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚ â”‚
â”‚  â”‚         â”‚                 â”‚                 â”‚              â”‚ â”‚
â”‚  â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚ â”‚
â”‚  â”‚                          â”‚                                 â”‚ â”‚
â”‚  â”‚                    IPC Bridge                              â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                               â”‚                                 â”‚
â”‚                        HTTP/REST API                            â”‚
â”‚                        (localhost:8000)                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      WSL2/LINUX ENVIRONMENT                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚                     FASTAPI BACKEND                        â”‚ â”‚
â”‚  â”‚                                                            â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚ â”‚
â”‚  â”‚  â”‚   main.py    â”‚  â”‚         Services Layer          â”‚   â”‚ â”‚
â”‚  â”‚  â”‚  911 lines   â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚   â”‚ â”‚
â”‚  â”‚  â”‚              â”‚  â”‚  â”‚ config_service.py       â”‚    â”‚   â”‚ â”‚
â”‚  â”‚  â”‚ API Routes   â”‚  â”‚  â”‚ (22KB - API keys,       â”‚    â”‚   â”‚ â”‚
â”‚  â”‚  â”‚ (22 endpoints)â”‚ â”‚  â”‚  encryption, settings)  â”‚    â”‚   â”‚ â”‚
â”‚  â”‚  â”‚              â”‚  â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    â”‚   â”‚ â”‚
â”‚  â”‚  â”‚ Request      â”‚  â”‚  â”‚ llm_service.py          â”‚    â”‚   â”‚ â”‚
â”‚  â”‚  â”‚ Validation   â”‚  â”‚  â”‚ (29KB - Multi-provider  â”‚    â”‚   â”‚ â”‚
â”‚  â”‚  â”‚              â”‚  â”‚  â”‚  AI integration)        â”‚    â”‚   â”‚ â”‚
â”‚  â”‚  â”‚ Error        â”‚  â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    â”‚   â”‚ â”‚
â”‚  â”‚  â”‚ Handling     â”‚  â”‚  â”‚ transcription_service.pyâ”‚    â”‚   â”‚ â”‚
â”‚  â”‚  â”‚              â”‚  â”‚  â”‚ (1.1KB - Whisper)       â”‚    â”‚   â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    â”‚   â”‚ â”‚
â”‚  â”‚         â”‚          â”‚  â”‚ vector_db_service.py    â”‚    â”‚   â”‚ â”‚
â”‚  â”‚         â”‚          â”‚  â”‚ (5KB - ChromaDB)        â”‚    â”‚   â”‚ â”‚
â”‚  â”‚         â”‚          â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    â”‚   â”‚ â”‚
â”‚  â”‚         â”‚          â”‚  â”‚ export_service.py       â”‚    â”‚   â”‚ â”‚
â”‚  â”‚         â”‚          â”‚  â”‚ (4.9KB - File exports)  â”‚    â”‚   â”‚ â”‚
â”‚  â”‚         â”‚          â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    â”‚   â”‚ â”‚
â”‚  â”‚         â”‚          â”‚  â”‚ downloader_service.py   â”‚    â”‚   â”‚ â”‚
â”‚  â”‚         â”‚          â”‚  â”‚ (1.2KB - URL downloads) â”‚    â”‚   â”‚ â”‚
â”‚  â”‚         â”‚          â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    â”‚   â”‚ â”‚
â”‚  â”‚         â”‚          â”‚  â”‚ tts_service.py          â”‚    â”‚   â”‚ â”‚
â”‚  â”‚         â”‚          â”‚  â”‚ (454 bytes - gTTS)      â”‚    â”‚   â”‚ â”‚
â”‚  â”‚         â”‚          â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚   â”‚ â”‚
â”‚  â”‚         â”‚          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚ â”‚
â”‚  â”‚         â”‚                                                 â”‚ â”‚
â”‚  â”‚         â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚ â”‚
â”‚  â”‚         â”‚                  â”‚                  â”‚          â”‚ â”‚
â”‚  â”‚   â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”   â”‚ â”‚
â”‚  â”‚   â”‚  Database  â”‚   â”‚   Vector    â”‚   â”‚  AI Models  â”‚   â”‚ â”‚
â”‚  â”‚   â”‚  (SQLite)  â”‚   â”‚   Database  â”‚   â”‚  (Whisper)  â”‚   â”‚ â”‚
â”‚  â”‚   â”‚            â”‚   â”‚  (ChromaDB) â”‚   â”‚             â”‚   â”‚ â”‚
â”‚  â”‚   â”‚ models.py  â”‚   â”‚             â”‚   â”‚ large-v3    â”‚   â”‚ â”‚
â”‚  â”‚   â”‚ database.pyâ”‚   â”‚ Embeddings  â”‚   â”‚             â”‚   â”‚ â”‚
â”‚  â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚           External AI Providers (via APIs)             â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚
â”‚  â”‚  â”‚ Ollama  â”‚  â”‚ OpenAI  â”‚  â”‚ Anthropic â”‚  â”‚ Google â”‚ â”‚ â”‚
â”‚  â”‚  â”‚ (Local) â”‚  â”‚         â”‚  â”‚  Claude   â”‚  â”‚ Gemini â”‚ â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
             â”‚      Local File System Storage     â”‚
             â”‚  ~/.local/share/InsightsLM/       â”‚
             â”‚  â”œâ”€â”€ insightslm.db (780KB)        â”‚
             â”‚  â”œâ”€â”€ chroma_db/ (11MB)            â”‚
             â”‚  â”œâ”€â”€ config.json (4KB, encrypted) â”‚
             â”‚  â”œâ”€â”€ temp_uploads/                â”‚
             â”‚  â””â”€â”€ static/                      â”‚
             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ’» Technology Stack

### **Frontend (Windows)**

| Technology | Version | Purpose | Lines of Code |
|-----------|---------|---------|---------------|
| **Electron** | 38.2.2 | Desktop app framework | - |
| **React** | 19.2.0 | UI library | 1,848 (App.tsx) |
| **TypeScript** | 4.5.5 | Type safety | ~2,900 total |
| **Vite** | 5.4.21 | Build tool | - |
| **Axios** | 1.12.2 | HTTP client | 211 (api.ts) |
| **Node.js** | 22.17.1 | Runtime | - |
| **npm** | 11.6.2 | Package manager | - |

**Key Files:**
- `main.ts` (824 lines) - Electron main process
- `App.tsx` (1,848 lines) - Main React application
- `api.ts` (211 lines) - Backend API client
- `preload.ts` (30 lines) - Preload script for IPC
- `log.ts` (new) - Logging utilities
- `renderer.tsx` (10 lines) - React entry point

### **Backend (WSL2/Linux)**

| Technology | Version | Purpose | Size/Lines |
|-----------|---------|---------|------------|
| **Python** | 3.12.3 | Runtime | - |
| **FastAPI** | 0.118.3 | Web framework | 911 (main.py) |
| **Uvicorn** | 0.37.0 | ASGI server | - |
| **Pydantic** | 2.12.0 | Data validation | - |
| **SQLAlchemy** | 2.0.44 | Database ORM | - |
| **OpenAI Whisper** | 20250625 | Speech-to-text | - |
| **ChromaDB** | 1.1.1 | Vector database | - |
| **sentence-transformers** | 5.1.1 | Text embeddings | - |

**AI Provider SDKs:**
- **ollama** 0.6.0 - Local AI models
- **openai** 2.3.0 - GPT models
- **anthropic** 0.69.0 - Claude models
- **google-generativeai** 0.8.5 - Gemini models

**Audio Processing:**
- **ffmpeg-python** 0.2.0 - Audio conversion
- **yt-dlp** 2025.10.22 - URL downloads
- **gTTS** 2.5.4 - Text-to-speech

**Security:**
- **pycryptodome** 3.23.0 - Encryption (AES-256)

**Key Files:**
- `main.py` (911 lines) - FastAPI application with 22 endpoints
- `config_service.py` (22KB) - Configuration and encryption
- `llm_service.py` (29KB) - Multi-provider AI integration
- `vector_db_service.py` (5KB) - Semantic search
- `transcription_service.py` (1.1KB) - Whisper integration
- `export_service.py` (4.9KB) - Export formats
- `downloader_service.py` (1.2KB) - URL downloads
- `tts_service.py` (454 bytes) - Text-to-speech
- `models.py` (1.7KB) - Database models
- `database.py` (1.5KB) - Database connection
- `schemas.py` - Pydantic request/response models

---

## ğŸ§© Component Architecture

### **Frontend Components**

#### **1. Main Process (main.ts - 824 lines)**
**Responsibilities:**
- Create and manage browser windows
- Handle backend lifecycle (spawn, health checks, port management)
- IPC communication with renderer
- System tray and menu management
- Auto-update functionality

**Key Functions:**
```typescript
createWindow()           // Create main application window
startBackend()          // Start FastAPI backend server
healthCheck()           // Verify backend availability
handleDeepLink()        // Handle custom protocol URLs
createApplicationMenu() // Platform-specific menus
```

#### **2. Renderer Process (App.tsx - 1,848 lines)**
**Responsibilities:**
- User interface rendering
- State management
- File upload handling
- Real-time updates (transcription, analysis)
- Settings management
- Audio playback controls

**State Management:**
```typescript
// Transcription state
const [transcriptionId, setTranscriptionId] = useState<number | null>(null);
const [segments, setSegments] = useState<TranscriptSegment[]>([]);
const [isLoading, setIsLoading] = useState(false);

// Settings state
const [llmConfig, setLlmConfig] = useState<Config | null>(null);
const [availableModels, setAvailableModels] = useState<ModelsResponse | null>(null);

// UI state
const [activeTab, setActiveTab] = useState<string>('chat');
const [error, setError] = useState<string>('');
```

**Component Sections:**
- Audio player with timeline
- Transcript display with timestamp navigation
- Chat interface for Q&A
- Summary generation
- Template management
- Report generation
- Settings panel
- Export options

#### **3. API Service (api.ts - 211 lines)**
**Responsibilities:**
- HTTP client for backend communication
- Request/response type definitions
- Error handling and retries
- Port configuration

**Exported Functions:** (25 total)
```typescript
// Transcription
uploadAndTranscribe()
transcribeFromUrl()

// Analysis
getSummary()
postQuery()
getAudioOverview()
runReport()

// Templates
getTemplates()
createTemplate()
updateTemplate()
deleteTemplate()

// Configuration
getConfig()
updateConfig()
testApiConnection()
getAllApiStatus()

// Models
getOllamaModels()
getOpenAIModels()
getAnthropicModels()
getGoogleModels()
getAllModels()

// Export
exportContent()
```

#### **4. Preload Script (preload.ts - 30 lines)**
**Responsibilities:**
- Bridge between main and renderer processes
- Secure IPC exposure
- Context isolation

#### **5. Logging Module (log.ts - NEW)**
**Responsibilities:**
- Cross-platform log formatting
- CRLF/LF detection and normalization
- Consistent logging across PowerShell and WSL

---

### **Backend Components**

#### **1. Main Application (main.py - 911 lines)**
**Responsibilities:**
- FastAPI app initialization
- CORS configuration
- Request routing (22 endpoints)
- Error handling and logging
- Dependency injection
- Static file serving

**API Endpoint Categories:**
```python
# Health & Testing (3 endpoints)
GET  /health
GET  /test-api/status
POST /test-api/{provider}

# Models (5 endpoints)
GET /models/ollama
GET /models/openai
GET /models/anthropic
GET /models/google
GET /models/all

# Transcription (4 endpoints)
POST /upload/
POST /download/
GET  /sources/
GET  /sources/{source_id}/transcription/

# Templates (4 endpoints)
GET    /templates/
POST   /templates/
PUT    /templates/{template_id}
DELETE /templates/{template_id}

# Analysis (4 endpoints)
POST /report/
POST /summarize/
POST /query/
POST /audio-overview/

# Export & Config (2 endpoints)
POST /export/
GET  /config/
PUT  /config/
```

#### **2. Configuration Service (config_service.py - 22KB)**
**Responsibilities:**
- Load/save encrypted configuration
- API key encryption/decryption (AES-256)
- Master key generation and storage
- Provider credential management
- Default configuration setup

**Key Functions:**
```python
load_config()              # Load and decrypt config
save_config()              # Encrypt and save config
encrypt_api_key()          # AES-256 encryption
decrypt_api_key()          # AES-256 decryption
get_master_key()           # Retrieve encryption key
generate_master_key()      # Create new master key
test_ollama_connection()   # Test Ollama availability
test_openai_connection()   # Test OpenAI API
test_anthropic_connection()# Test Anthropic API
test_google_connection()   # Test Google Gemini API
get_all_api_status()       # Test all providers
```

**Configuration Schema:**
```python
{
    "ollama_base_url": "http://localhost:11434",
    "ollama_api_key_encrypted": "...",
    "openai_api_key_encrypted": "...",
    "openai_model": "gpt-4o",
    "anthropic_api_key_encrypted": "...",
    "anthropic_model": "claude-sonnet-4",
    "google_api_key_encrypted": "...",
    "google_model": "gemini-1.5-pro",
    "default_provider": "ollama"
}
```

#### **3. LLM Service (llm_service.py - 29KB)**
**Responsibilities:**
- Multi-provider AI model integration
- Dynamic model discovery
- Text generation with streaming
- Provider-specific error handling
- Token usage tracking

**Supported Providers:**
- **Ollama** (local models) - 45+ models
- **OpenAI** (GPT-3.5, GPT-4) - 21 models
- **Anthropic** (Claude 3, Claude 4) - 9 models
- **Google** (Gemini 1.5) - 29+ models

**Key Functions:**
```python
get_ollama_models()        # Discover available Ollama models
get_openai_models()        # List OpenAI models
get_anthropic_models()     # List Anthropic models
get_google_models()        # List Google Gemini models
get_all_models()           # Aggregate all providers
generate_text()            # Generate AI response
generate_text_with_provider() # Provider-specific generation
```

**Error Handling:**
- API key validation
- Rate limiting detection
- Network error recovery
- User-friendly error messages

#### **4. Transcription Service (transcription_service.py - 1.1KB)**
**Responsibilities:**
- OpenAI Whisper integration
- Audio transcription with timestamps
- Language detection
- Segment extraction

**Key Functions:**
```python
transcribe_audio(file_path: str, language: str = "auto") -> dict
```

**Output Format:**
```python
{
    "text": "Full transcript...",
    "segments": [
        {
            "start": 0.0,
            "end": 5.2,
            "text": "Segment text..."
        },
        ...
    ],
    "language": "en"
}
```

#### **5. Vector Database Service (vector_db_service.py - 5KB)**
**Responsibilities:**
- ChromaDB integration
- Text embedding generation
- Semantic search
- Collection management

**Key Functions:**
```python
add_to_vector_db()         # Store embeddings
query_vector_db()          # Semantic search
delete_from_vector_db()    # Remove embeddings
```

#### **6. Export Service (export_service.py - 4.9KB)**
**Responsibilities:**
- Multi-format export (TXT, MD, DOCX, PDF, JSON)
- Template-based formatting
- File generation

**Supported Formats:**
- **TXT** - Plain text
- **MD** - Markdown
- **DOCX** - Microsoft Word
- **PDF** - Portable Document Format
- **JSON** - Structured data

#### **7. Downloader Service (downloader_service.py - 1.2KB)**
**Responsibilities:**
- URL audio download
- YouTube video extraction
- Format conversion

**Key Functions:**
```python
download_audio_from_url(url: str) -> str
```

#### **8. TTS Service (tts_service.py - 454 bytes)**
**Responsibilities:**
- Text-to-speech conversion
- gTTS integration
- Audio file generation

**Key Functions:**
```python
text_to_speech(text: str, output_path: str, language: str = 'en')
```

---

## ğŸ”„ Data Flow

### **1. Audio Transcription Flow**

```
User Uploads Audio File
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Frontend (App.tsx) â”‚
â”‚  File validation    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚ HTTP POST /upload/
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Backend (main.py)      â”‚
â”‚  Route: /upload/        â”‚
â”‚  - Save to temp_uploads â”‚
â”‚  - Validate file        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Transcription Service      â”‚
â”‚  - Load Whisper large-v3    â”‚
â”‚  - Transcribe audio         â”‚
â”‚  - Extract segments         â”‚
â”‚  - Detect language          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Database (SQLite)          â”‚
â”‚  - Insert Source record     â”‚
â”‚  - Store transcription      â”‚
â”‚  - Save metadata            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Vector DB Service          â”‚
â”‚  - Generate embeddings      â”‚
â”‚  - Store in ChromaDB        â”‚
â”‚  - Index for search         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Response to Frontendâ”‚
â”‚  - source_id         â”‚
â”‚  - transcription     â”‚
â”‚  - segments          â”‚
â”‚  - language          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### **2. AI Analysis Flow (Summary, Chat, Report)**

```
User Requests Analysis
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Frontend (App.tsx) â”‚
â”‚  - Select model     â”‚
â”‚  - Enter query/     â”‚
â”‚    template         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚ HTTP POST /summarize/ or /query/ or /report/
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Backend (main.py)      â”‚
â”‚  Route handler          â”‚
â”‚  - Validate source_id   â”‚
â”‚  - Validate model_key   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Database Service           â”‚
â”‚  - Fetch transcription      â”‚
â”‚  - Load template (if report)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Vector DB Service          â”‚
â”‚  (Optional - for context)   â”‚
â”‚  - Semantic search          â”‚
â”‚  - Retrieve relevant chunks â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LLM Service                â”‚
â”‚  - Parse model_key          â”‚
â”‚  - Select provider          â”‚
â”‚  - Build prompt             â”‚
â”‚  - Call AI API              â”‚
â”‚  - Stream response          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Response to Frontendâ”‚
â”‚  - AI-generated text â”‚
â”‚  - Streaming updates â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### **3. Configuration Update Flow**

```
User Updates Settings
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Frontend (App.tsx) â”‚
â”‚  Settings Panel     â”‚
â”‚  - Enter API keys   â”‚
â”‚  - Select models    â”‚
â”‚  - Test connections â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚ HTTP PUT /config/
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Backend (main.py)      â”‚
â”‚  Route: /config/        â”‚
â”‚  - Validate config      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Config Service             â”‚
â”‚  - Encrypt API keys         â”‚
â”‚    (AES-256)                â”‚
â”‚  - Generate master key      â”‚
â”‚    (if first time)          â”‚
â”‚  - Save to config.json      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  File System                â”‚
â”‚  ~/.local/share/InsightsLM/ â”‚
â”‚  â”œâ”€â”€ config.json (encrypted)â”‚
â”‚  â””â”€â”€ .encryption_key        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Response to Frontendâ”‚
â”‚  - Success/failure   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ—„ï¸ Database Design

### **SQLite Database**

**Location:** `~/.local/share/InsightsLM/insightslm.db`  
**Size:** ~780KB  
**ORM:** SQLAlchemy 2.0.44

#### **Tables**

**1. sources**
```sql
CREATE TABLE sources (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    title VARCHAR(500),
    filename VARCHAR(500),
    filepath VARCHAR(1000),
    file_size INTEGER,
    duration FLOAT,
    language VARCHAR(10),
    transcription_text TEXT,
    transcription_segments JSON,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

**Purpose:** Store uploaded/downloaded audio files and their transcriptions

**2. templates**
```sql
CREATE TABLE templates (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    name VARCHAR(200) UNIQUE NOT NULL,
    prompt_text TEXT NOT NULL,
    language VARCHAR(10) DEFAULT 'en',
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

**Purpose:** Store custom report templates

**3. reports** (potential future table)
```sql
CREATE TABLE reports (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    source_id INTEGER REFERENCES sources(id),
    template_id INTEGER REFERENCES templates(id),
    content TEXT,
    model_used VARCHAR(100),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

**Purpose:** Cache generated reports

#### **Entity-Relationship Diagram**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    sources       â”‚         â”‚    templates     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤         â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ id (PK)          â”‚         â”‚ id (PK)          â”‚
â”‚ title            â”‚         â”‚ name (UNIQUE)    â”‚
â”‚ filename         â”‚         â”‚ prompt_text      â”‚
â”‚ filepath         â”‚         â”‚ language         â”‚
â”‚ file_size        â”‚         â”‚ created_at       â”‚
â”‚ duration         â”‚         â”‚ updated_at       â”‚
â”‚ language         â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚ transcription    â”‚                â”‚
â”‚ segments (JSON)  â”‚                â”‚
â”‚ created_at       â”‚                â”‚
â”‚ updated_at       â”‚                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
         â”‚                          â”‚
         â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
         â””â”€â”€â”‚     reports        â”‚â”€â”€â”˜
            â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
            â”‚ id (PK)            â”‚
            â”‚ source_id (FK)     â”‚
            â”‚ template_id (FK)   â”‚
            â”‚ content            â”‚
            â”‚ model_used         â”‚
            â”‚ created_at         â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### **ChromaDB (Vector Database)**

**Location:** `~/.local/share/InsightsLM/chroma_db/`  
**Size:** ~11MB  
**Version:** 1.1.1

#### **Collections**

**transcription_chunks**
```python
{
    "documents": ["Transcript segment text..."],
    "embeddings": [[0.123, 0.456, ...]],  # 384-dimensional vectors
    "metadatas": [{
        "source_id": 123,
        "segment_index": 0,
        "start_time": 0.0,
        "end_time": 5.2,
        "speaker": "Speaker 1"
    }],
    "ids": ["source_123_segment_0"]
}
```

**Purpose:** Enable semantic search across transcriptions

**Embedding Model:** sentence-transformers/all-MiniLM-L6-v2 (384 dimensions)

---

## ğŸ”’ Security Architecture

### **1. API Key Encryption**

**Algorithm:** AES-256-CBC  
**Library:** pycryptodome 3.23.0

```python
# Encryption Flow
User enters API key
    â†“
Generate/Load master key (32 bytes)
    â†“
Derive encryption key using PBKDF2
    â†“
Encrypt API key with AES-256
    â†“
Store in config.json (encrypted)
```

**Master Key Storage:**
- Location: `~/.local/share/InsightsLM/.encryption_key`
- Permissions: `600` (owner read/write only)
- Generated once on first configuration

**Configuration File:**
- Location: `~/.local/share/InsightsLM/config.json`
- Permissions: `644` (owner read/write, others read)
- Contains only encrypted API keys

### **2. IPC Security (Electron)**

**Context Isolation:** Enabled  
**Node Integration:** Disabled in renderer  
**Preload Script:** Whitelisted APIs only

```typescript
// preload.ts exposes only safe APIs
contextBridge.exposeInMainWorld('electronAPI', {
    setBackendPort: (port: number) => ipcRenderer.send('set-backend-port', port),
    // No filesystem or process access exposed
});
```

### **3. File System Security**

**Temp Uploads:**
- Location: `~/.local/share/InsightsLM/temp_uploads/`
- Auto-cleanup after processing
- Random UUID filenames

**Static Files:**
- Served only from designated directory
- No directory traversal allowed
- Proper MIME type validation

### **4. Network Security**

**CORS Configuration:**
```python
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:*"],  # Only local frontend
    allow_methods=["*"],
    allow_headers=["*"],
)
```

**Backend Binding:**
- Listens on `0.0.0.0:8000` (all interfaces)
- Only accessible via localhost (WSL2 networking)

---

## ğŸš€ Deployment Architecture

### **Development Environment**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          Developer Machine              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚         Windows 10 Pro             â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚ â”‚
â”‚  â”‚  â”‚    Frontend Development      â”‚  â”‚ â”‚
â”‚  â”‚  â”‚  npm run start               â”‚  â”‚ â”‚
â”‚  â”‚  â”‚  (Hot reload enabled)        â”‚  â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚       WSL2 (Ubuntu 24.04)          â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚ â”‚
â”‚  â”‚  â”‚    Backend Development       â”‚  â”‚ â”‚
â”‚  â”‚  â”‚  uvicorn main:app --reload   â”‚  â”‚ â”‚
â”‚  â”‚  â”‚  (Auto-reload enabled)       â”‚  â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### **Production Build**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        Electron Forge Build             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  npm run make                      â”‚ â”‚
â”‚  â”‚  â”œâ”€â”€ Windows (.exe installer)     â”‚ â”‚
â”‚  â”‚  â”œâ”€â”€ macOS (.dmg)                 â”‚ â”‚
â”‚  â”‚  â”œâ”€â”€ Linux (.deb, .rpm)           â”‚ â”‚
â”‚  â”‚  â””â”€â”€ Portable (.zip)              â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                         â”‚
â”‚  Backend bundled with:                  â”‚
â”‚  â”œâ”€â”€ Python runtime                     â”‚
â”‚  â”œâ”€â”€ Virtual environment                â”‚
â”‚  â”œâ”€â”€ All dependencies                   â”‚
â”‚  â””â”€â”€ Whisper model (large-v3)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Build Output:**
- Installers for Windows, macOS, Linux
- Self-contained executables
- Auto-update capable (Squirrel)

---

## ğŸ¤” Design Decisions

### **1. Why Hybrid Architecture (Electron + FastAPI)?**

**Decision:** Separate frontend (Electron) from backend (FastAPI)

**Rationale:**
- **Performance:** Heavy AI processing doesn't block UI
- **Flexibility:** Can run backend on different machine/GPU server
- **Isolation:** Security boundary between UI and AI APIs
- **Development:** Independent frontend/backend development
- **Testing:** Easier to test API endpoints

**Trade-offs:**
- More complex than single-process app
- Requires backend lifecycle management
- Network communication overhead (minimal on localhost)

---

### **2. Why WSL2 for Backend?**

**Decision:** Run backend in WSL2 (Linux) instead of native Windows

**Rationale:**
- **GPU Support:** Better CUDA/PyTorch support in Linux
- **Performance:** Whisper runs faster on Linux
- **Dependencies:** Easier to install Python packages
- **Consistency:** Production-like environment
- **Tools:** Better CLI tools (ffmpeg, yt-dlp)

**Trade-offs:**
- Requires WSL2 installation
- Slightly more complex setup
- Windows-only deployment (currently)

---

### **3. Why SQLite + ChromaDB?**

**Decision:** Use SQLite for structured data, ChromaDB for vector search

**Rationale:**
- **SQLite:**
  - No server required
  - Single file database
  - Fast for small-medium datasets
  - Built-in Python support
  
- **ChromaDB:**
  - Lightweight vector database
  - No separate server needed
  - Easy integration with sentence-transformers
  - Excellent semantic search performance

**Trade-offs:**
- Not suitable for massive datasets (>10GB)
- No built-in replication/clustering
- Good enough for desktop application

---

### **4. Why Multi-Provider AI?**

**Decision:** Support 4 AI providers instead of single vendor

**Rationale:**
- **Choice:** Users select model based on needs
- **Cost:** Ollama is free, others are paid
- **Privacy:** Local models (Ollama) keep data private
- **Capability:** Different models excel at different tasks
- **Resilience:** Fallback if one provider is down

**Supported Providers:**
1. **Ollama** - Local, free, privacy-focused
2. **OpenAI** - GPT-4, best general performance
3. **Anthropic** - Claude, excellent for analysis
4. **Google Gemini** - Large context windows

---

### **5. Why AES-256 Encryption for API Keys?**

**Decision:** Encrypt API keys at rest with AES-256

**Rationale:**
- **Security:** Protect API keys from unauthorized access
- **Best Practice:** Industry standard encryption
- **Compliance:** Meets security requirements
- **Master Key:** Securely stored separately

**Implementation:**
```python
from Crypto.Cipher import AES
from Crypto.Protocol.KDF import PBKDF2
from Crypto.Random import get_random_bytes

def encrypt_api_key(api_key: str, master_key: bytes) -> str:
    salt = get_random_bytes(16)
    key = PBKDF2(master_key, salt, dkLen=32)
    cipher = AES.new(key, AES.MODE_CBC)
    ciphertext = cipher.encrypt(pad(api_key.encode(), AES.block_size))
    return base64.b64encode(salt + cipher.iv + ciphertext).decode()
```

---

### **6. Why React Over Vue/Angular?**

**Decision:** Use React for frontend UI

**Rationale:**
- **Ecosystem:** Largest component library ecosystem
- **Performance:** Virtual DOM is fast enough
- **Popularity:** Easier to find developers/resources
- **Integration:** Works well with Electron
- **TypeScript:** Excellent TypeScript support

---

### **7. Why Whisper large-v3?**

**Decision:** Use OpenAI Whisper large-v3 model for transcription

**Rationale:**
- **Accuracy:** Best open-source transcription model
- **Languages:** Supports 99 languages
- **Speed:** Fast enough with GPU (30x real-time)
- **Cost:** Free, open-source
- **Timestamps:** Provides word-level timestamps

**Trade-offs:**
- Large model (~3GB)
- Requires GPU for good performance
- CPU transcription is slow (~1x real-time)

---

## ğŸ”® Future Architectural Considerations

### **Potential Enhancements**

1. **Microservices Split**
   - Separate transcription service
   - Separate AI analysis service
   - Better scalability

2. **Cloud Sync**
   - Optional cloud backup
   - Multi-device sync
   - Shared templates

3. **Plugin System**
   - Third-party integrations
   - Custom AI providers
   - Export format plugins

4. **Real-time Collaboration**
   - Multi-user editing
   - Shared workspaces
   - Live transcription

5. **Mobile Clients**
   - React Native mobile app
   - Shared backend
   - Cloud sync required

6. **Advanced Analytics**
   - Sentiment analysis
   - Speaker diarization
   - Topic modeling
   - Trend analysis

---

## ğŸ“š References

**Frontend:**
- Electron Documentation: https://www.electronjs.org/docs
- React Documentation: https://react.dev
- TypeScript Handbook: https://www.typescriptlang.org/docs

**Backend:**
- FastAPI Documentation: https://fastapi.tiangolo.com
- SQLAlchemy Documentation: https://docs.sqlalchemy.org
- ChromaDB Documentation: https://docs.trychroma.com

**AI/ML:**
- OpenAI Whisper: https://github.com/openai/whisper
- Ollama: https://ollama.ai
- Anthropic Claude: https://www.anthropic.com/claude
- Google Gemini: https://ai.google.dev

---

**Document Version:** 1.1  
**Last Updated:** November 4, 2025  
**Status:** âœ… Production-Ready - Updated with actual system information
